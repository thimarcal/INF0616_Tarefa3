{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF-616 - Tarefa 3 - *OvO*/*OvR* e teste de hipótese nula\n",
    "\n",
    "Professor: Jacques Wainer -- wainer@ic.unicamp.br  \n",
    "Monitor: Lucas David -- ra188972@students.ic.unicamp.br\n",
    "\n",
    "Instituto de Computação - Unicamp  \n",
    "2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "x, y = digits.data, digits.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('# de amostras de treino: %i' % x_train.shape[0],\n",
    "      '# de amostras de teste: %i' % x_test.shape[0],\n",
    "      '# de características: %s' % x_train.shape[1],\n",
    "      '# de classes: %i' % (np.max(y_train) + 1),\n",
    "      sep='\\n', end='\\n\\n')\n",
    "\n",
    "print('Algumas amostras de dígitos:')\n",
    "_ = plt.figure(figsize=(16, 8))\n",
    "for ix in range(8  * 32):\n",
    "    plt.subplot(8, 32, ix + 1)\n",
    "    plt.imshow(x_train[ix].reshape(8, 8), cmap='Greys')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abaixo estão algumas funções que podem ser úteis no decorrer dessa atividade:\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Originally implemented at: <http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html>\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('classe verdadeira')\n",
    "    plt.xlabel('class predita')\n",
    "\n",
    "\n",
    "def train_and_evaluate(tag, model, train, test):\n",
    "    \"\"\"Treina um modelo `model` sobre o conjunto `train=(x, y)` e avalia\n",
    "       os resultados sobre o conjunto `test`.\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = train, test\n",
    "    p_test = model.fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "    print('~> Strategy', tag)\n",
    "    print(metrics.classification_report(y_test, p_test, target_names=digits.target_names.astype(str)))\n",
    "\n",
    "    plot_confusion_matrix(metrics.confusion_matrix(test[1], p_test),\n",
    "                          digits.target_names.astype(str),\n",
    "                          title='Matriz de confusão sobre Digits/teste c/ a estratégia %s' % tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OvO e OvR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por padrão, os classificadores do sklearn assumem a estratégia one-vs-one. Utilize a função `train_and_evaluate` acima, passando um SVC linear convencional usando os hiperparametros default (nesse caso C=1), estudado até agora, e observe a pontuação sobre o conjunto de teste.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate a \"conventional\" model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para se adotar uma estratégia diferente, é necessário a utilização de algum dos *wrappers* em `sklearn.multiclass`. Utilize as classes adequadas para corretamente executar as abordagens OvO e OvR. Treine e avalie os resultados sobre *digits*.**\n",
    "\n",
    "Garanta que ambos os classificadores começem com o mesmo estado aleatório `rs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 571\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "\n",
    "ovo_model = OneVsOneClassifier(SVC(kernel='linear', random_state=rs))\n",
    "ovr_model = OneVsRestClassifier(SVC(kernel='linear', random_state=rs))\n",
    "\n",
    "# Treinar e avaliar ambos modelos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados obtidos na célula acima são suficientes para afirmar que algum dos\n",
    "classificadores (OvO e OvR) é *provavelmente* melhor que o outro sobre o conjunto\n",
    "*digits*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos rodar `OvO` vs `OvA`, mas usando 5 repedições de 2 *folds*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, RepeatedKFold\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=2, n_repeats=5, random_state=41)\n",
    "\n",
    "ovo_model = OneVsOneClassifier(SVC(kernel='linear', random_state=rs))\n",
    "ovo=cross_validate(ovo_model,x,y,cv=cv)\n",
    "\n",
    "# ovr = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é a melhor estrategia? Use a mediana dos `test_score` do dicionário resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique se a diferença do `test_score` entre `OvO` e `OvR` é estatisticamente signifidante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Testar e reportar p-valor..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação estatística entre GBMs OvO e OvR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sejam `ovo_model` e `ovr_model` modelos `GBM` (ou `GradientBoostingClassifier`) que se apoiam sobre as estratégias OvO e OvR, respectivamente. Utilize 5x2 CV a fim de otimizar os hiper-parâmetros e construir duas distribuições de pontuações sobre as folds de validação formadas, respeitando as seguintes restrições:**\n",
    "\n",
    "- Busque na grade de hiper-parâmetros do estimador pela combinação que maximiza a pontuação f1-macro média sobre as folds de validação.\n",
    "- Garanta que ao menos os parâmetros *número de árvores* e *profundidade dos ramos* pertencam à sua grade de busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "params = dict(estimator__n_estimators=[10, 20, 30],\n",
    "              estimator__max_depth=[2, 3, 5])\n",
    "gbc = GradientBoostingClassifier(random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovo_model=GridSearchCV(OneVsOneClassifier(gbc),params,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=2, n_repeats=5, random_state=41)\n",
    "\n",
    "# Vai demorar vários minutos para rodar a linha abaixo:\n",
    "ovo = cross_validate(ovo_model,x,y,cv=cv)\n",
    "ovo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar OvR..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual o melhor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('OvO:', ...)\n",
    "# print('OvR:', ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
